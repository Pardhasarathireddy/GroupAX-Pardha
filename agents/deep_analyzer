"""
Deep Code Analysis Agent - Production Version
Accurate scoring for all 7 dimensions + rich structured output
"""
import json
from typing import List, Dict, Any
from langchain_google_genai import ChatGoogleGenerativeAI
import re


class DeepCodeAnalyzer:
    def __init__(self, api_key: str):
        """Initialize the deep code analyzer"""
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            google_api_key=api_key,
            temperature=0.1,
            max_tokens=8192
        )
    
    def deep_analyze(self, files: List[Dict], repo_structure: str) -> Dict[str, Any]:
        """Perform comprehensive analysis across 7 dimensions"""
        print("🔍 Starting deep analysis...")
        
        results = {
            'security': self._analyze_security(files),
            'performance': self._analyze_performance(files),
            'architecture': self._analyze_architecture(files, repo_structure),
            'code_quality': self._analyze_code_quality(files),
            'documentation': self._analyze_documentation(files),
            'dependencies': self._analyze_dependencies(files),
            'best_practices': self._analyze_best_practices(files)
        }
        
        # Calculate scores for each dimension
        results['scores'] = self._calculate_all_scores(results)
        
        # Generate summary and priorities
        results['summary'] = self._generate_summary(results)
        results['priority_fixes'] = self._prioritize_issues(results)
        
        return results
    
    def _analyze_security(self, files: List[Dict]) -> Dict:
        """Deep security analysis with hybrid approach"""
        
        # STEP 1: Pattern-based detection (guaranteed to work)
        pattern_issues = self._detect_security_patterns(files)
        
        # STEP 2: LLM-based analysis (for deeper insights)
        code_context = self._prepare_code_context(files, max_files=20)
        
        if not code_context.strip():
            return {
                "critical_issues": pattern_issues['critical'],
                "high_severity": pattern_issues['high'],
                "medium_severity": [],
                "low_severity": [],
                "total_issues": len(pattern_issues['critical']) + len(pattern_issues['high'])
            }
        
        prompt = f"""Analyze this code for security vulnerabilities. Return findings as a JSON array.

    Code:
    {code_context[:15000]}

    Find these security issues:
    1. Hardcoded passwords, API keys, tokens
    2. SQL injection vulnerabilities
    3. Command injection risks
    4. XSS vulnerabilities
    5. Path traversal issues

    For each issue found, create an entry with:
    - file: the filename
    - issue: brief description
    - severity: CRITICAL, HIGH, MEDIUM, or LOW
    - description: detailed explanation
    - fix: how to fix it

    Return as JSON array:
    [
    {{
        "file": "app.py",
        "issue": "Hardcoded password",
        "severity": "CRITICAL",
        "description": "Password stored in plaintext",
        "fix": "Use environment variables"
    }}
    ]

    Return ONLY the JSON array, nothing else.
    """
        
        try:
            response = self.llm.invoke(prompt)
            content = response.content.strip()
            
            # Try to extract JSON array
            if '[' in content and ']' in content:
                start = content.find('[')
                end = content.rfind(']') + 1
                json_str = content[start:end]
                llm_issues = json.loads(json_str)
            else:
                # Fallback: parse narrative response
                llm_issues = self._parse_narrative_security(content)
            
            # Categorize LLM findings
            critical_llm = [i for i in llm_issues if i.get('severity') == 'CRITICAL']
            high_llm = [i for i in llm_issues if i.get('severity') == 'HIGH']
            medium_llm = [i for i in llm_issues if i.get('severity') == 'MEDIUM']
            low_llm = [i for i in llm_issues if i.get('severity') == 'LOW']
            
            # Combine with pattern-based findings
            result = {
                "critical_issues": pattern_issues['critical'] + critical_llm,
                "high_severity": pattern_issues['high'] + high_llm,
                "medium_severity": medium_llm,
                "low_severity": low_llm,
                "total_issues": 0
            }
            
            result['total_issues'] = sum(len(result[k]) for k in 
                ['critical_issues', 'high_severity', 'medium_severity', 'low_severity'])
            
            print(f"✅ Security: {result['total_issues']} total issues found")
            return result
            
        except Exception as e:
            print(f"⚠ LLM parsing failed: {e}, using pattern detection only")
            return {
                "critical_issues": pattern_issues['critical'],
                "high_severity": pattern_issues['high'],
                "medium_severity": [],
                "low_severity": [],
                "total_issues": len(pattern_issues['critical']) + len(pattern_issues['high'])
            }
    def _analyze_performance(self, files: List[Dict]) -> Dict:
        """Performance analysis with specific bottlenecks"""
        code_context = self._prepare_code_context(files, max_files=30)
        
        prompt = f"""Analyze this code for performance issues and bottlenecks.

    Code:
    {code_context}

    Find ALL performance problems:
    1. Inefficient algorithms (O(n²) or worse)
    2. N+1 database query problems
    3. Missing indexes or slow queries
    4. Memory leaks and excessive memory usage
    5. Blocking I/O operations
    6. Synchronous calls that should be async
    7. Unnecessary loops and iterations
    8. Inefficient data structures
    9. Missing caching opportunities
    10. Resource not being released

    For each issue provide:
    - File and location
    - Impact level: SEVERE, HIGH, MODERATE, LOW
    - Current Big-O complexity if applicable
    - Specific bottleneck description
    - Optimized approach with code example
    - Expected performance improvement

    Return ONLY valid JSON:
    {{
    "issues": [
        {{
        "file": "filename.py",
        "location": "function_name or line",
        "impact": "SEVERE",
        "issue": "Nested loops causing O(n²) complexity",
        "current_complexity": "O(n²)",
        "description": "Checking every item against every other item",
        "fix": "Use set or dict for O(1) lookups",
        "improvement": "100x faster for large datasets"
        }}
    ],
    "total_issues": 0
    }}
    """
        
        try:
            response = self.llm.invoke(prompt)
            content = self._clean_json_response(response.content)
            result = json.loads(content)
            
            if 'total_issues' not in result:
                result['total_issues'] = len(result.get('issues', []))
            
            return result
        except Exception as e:
            print(f"Performance analysis error: {e}")
            return {"issues": [], "total_issues": 0}

    def _detect_security_patterns(self, files: List[Dict]) -> Dict:
            """Pattern-based security detection (always works)"""
            critical = []
            high = []
            
            for file_info in files:
                if not file_info['path'].endswith(('.py', '.js', '.java', '.php')):
                    continue
                    
                content = file_info['content']
                filepath = file_info['path']
                
                # Critical: Hardcoded credentials
                if re.search(r'(password|passwd|pwd)\s*=\s*["\'][^"\']+["\']', content, re.IGNORECASE):
                    critical.append({
                        "file": filepath,
                        "issue": "Hardcoded password detected",
                        "severity": "CRITICAL",
                        "description": "Password stored in plaintext in source code",
                        "fix": "Use environment variables: os.getenv('PASSWORD')"
                    })
                
                if re.search(r'(api[-]?key|apikey|secret[-]?key)\s*=\s*["\'][^"\']+["\']', content, re.IGNORECASE):
                    critical.append({
                        "file": filepath,
                        "issue": "Hardcoded API key",
                        "severity": "CRITICAL",
                        "description": "API key exposed in source code",
                        "fix": "Use environment variables or secret manager"
                    })
                
                # Critical: SQL Injection
                if re.search(r'(execute|cursor\.execute|query)\s*\(\s*f["\']|format\s*\(', content):
                    critical.append({
                        "file": filepath,
                        "issue": "SQL injection vulnerability",
                        "severity": "CRITICAL",
                        "description": "String formatting used in SQL query",
                        "fix": "Use parameterized queries with placeholders"
                    })
                
                # High: Command Injection
                if re.search(r'os\.system|subprocess\.call.*shell=True', content):
                    high.append({
                        "file": filepath,
                        "issue": "Command injection risk",
                        "severity": "HIGH",
                        "description": "Direct command execution with user input",
                        "fix": "Use subprocess with shell=False and input validation"
                    })
                
                # High: Eval usage
                if re.search(r'\beval\s*\(', content):
                    high.append({
                        "file": filepath,
                        "issue": "Dangerous eval() usage",
                        "severity": "HIGH",
                        "description": "eval() can execute arbitrary code",
                        "fix": "Use ast.literal_eval() or JSON parsing"
                    })
            
            return {'critical': critical, 'high': high}

    def _parse_narrative_security(self, narrative: str) -> List[Dict]:
        """Parse narrative security response into structured format"""
        issues = []
        
        # Look for common issue indicators
        lines = narrative.split('\n')
        current_issue = {}
        
        for line in lines:
            line = line.strip()
            
            if not line:
                if current_issue:
                    issues.append(current_issue)
                    current_issue = {}
                continue
            
            # Detect severity
            if 'critical' in line.lower() or 'hardcoded' in line.lower():
                current_issue['severity'] = 'CRITICAL'
            elif 'high' in line.lower() or 'injection' in line.lower():
                current_issue['severity'] = 'HIGH'
            
            # Detect issue type
            if 'password' in line.lower():
                current_issue['issue'] = 'Hardcoded password'
                current_issue['description'] = line
            elif 'api' in line.lower() and 'key' in line.lower():
                current_issue['issue'] = 'Hardcoded API key'
                current_issue['description'] = line
            elif 'sql' in line.lower() and 'injection' in line.lower():
                current_issue['issue'] = 'SQL injection vulnerability'
                current_issue['description'] = line
            
            # Default values
            if 'issue' in current_issue and 'file' not in current_issue:
                current_issue['file'] = 'Multiple files'
                current_issue['fix'] = 'See description for details'
        
        if current_issue:
            issues.append(current_issue)
        
        return issues
        
    def _analyze_architecture(self, files: List[Dict], repo_structure: str) -> Dict:
            """Architecture and design analysis"""
            code_context = self._prepare_code_context(files, max_files=25)
            
            prompt = f"""Analyze the software architecture and design patterns.

    Repository Structure:
    {repo_structure}

    Code Sample:
    {code_context}

    Evaluate:
    1. Architecture pattern (MVC, MVVM, Clean Architecture, Layered, etc.)
    2. SOLID principles violations
    3. Separation of concerns
    4. Code organization and modularity
    5. Dependency management and coupling
    6. Design patterns used/missing
    7. Scalability concerns
    8. Maintainability issues

    Return ONLY valid JSON:
    {{
    "architecture_pattern": "MVC/Layered/Unknown",
    "issues": [
        {{
        "category": "SOLID Violation",
        "severity": "HIGH",
        "issue": "Single Responsibility Principle violated",
        "description": "Class handles both business logic and data persistence",
        "file": "filename.py",
        "recommendation": "Split into separate service and repository classes"
        }}
    ],
    "strengths": ["List any good architectural decisions"],
    "total_issues": 0
    }}
    """
            
            try:
                response = self.llm.invoke(prompt)
                content = self._clean_json_response(response.content)
                result = json.loads(content)
                
                if 'total_issues' not in result:
                    result['total_issues'] = len(result.get('issues', []))
                
                return result
            except Exception as e:
                print(f"Architecture analysis error: {e}")
                return {
                    "architecture_pattern": "Unknown",
                    "issues": [],
                    "strengths": [],
                    "total_issues": 0
                }
        
    def _analyze_code_quality(self, files: List[Dict]) -> Dict:
            """Code quality and maintainability analysis"""
            code_context = self._prepare_code_context(files, max_files=30)
            
            prompt = f"""Analyze code quality, style, and maintainability.

    Code:
    {code_context}

    Find issues in:
    1. Code smells (long methods, god classes, duplicated code)
    2. Cyclomatic complexity (too many branches)
    3. Cognitive complexity (hard to understand)
    4. Naming conventions (unclear, inconsistent)
    5. Magic numbers and hardcoded values
    6. Dead code and unused imports
    7. Error handling quality
    8. Code duplication
    9. Function length and parameter count
    10. Type safety and type hints

    Return ONLY valid JSON:
    {{
    "issues": [
        {{
        "file": "filename.py",
        "type": "code_smell",
        "severity": "MEDIUM",
        "issue": "Function too long (150 lines)",
        "description": "Function handles multiple responsibilities",
        "suggestion": "Break into smaller, focused functions",
        "lines": "50-200"
        }}
    ],
    "metrics": {{
        "avg_function_length": 0,
        "max_complexity": 0,
        "duplication_percentage": 0
    }},
    "total_issues": 0
    }}
    """
            
            try:
                response = self.llm.invoke(prompt)
                content = self._clean_json_response(response.content)
                result = json.loads(content)
                
                if 'total_issues' not in result:
                    result['total_issues'] = len(result.get('issues', []))
                
                return result
            except Exception as e:
                print(f"Code quality analysis error: {e}")
                return {
                    "issues": [],
                    "metrics": {},
                    "total_issues": 0
                }
        
    def _analyze_documentation(self, files: List[Dict]) -> Dict:
            """Documentation coverage and quality analysis"""
            total_functions = 0
            documented_functions = 0
            files_needing_docs = []
            
            for file_info in files:
                if not file_info['path'].endswith('.py'):
                    continue
                    
                content = file_info['content']
                
                # Find all function definitions
                functions = re.findall(r'def\s+(\w+)\s*\([^)]*\):', content)
                total_functions += len(functions)
                
                # Check for docstrings
                for func in functions:
                    # Look for docstring after function definition
                    pattern = rf'def\s+{func}\s*\([^)]\):\s(?:"""|\'\'\')[^"\']*(?:"""|\'\'\')'
                    if re.search(pattern, content, re.DOTALL):
                        documented_functions += 1
                    else:
                        files_needing_docs.append({
                            "file": file_info['path'],
                            "function": func,
                            "issue": "Missing docstring",
                            "severity": "MEDIUM"
                        })
            
            # Calculate coverage
            coverage = int((documented_functions / total_functions * 100)) if total_functions > 0 else 100
            
            return {
                "coverage_percentage": coverage,
                "total_functions": total_functions,
                "documented_functions": documented_functions,
                "undocumented_functions": total_functions - documented_functions,
                "issues": files_needing_docs[:20],
                "total_issues": len(files_needing_docs),
                "files_analyzed": len([f for f in files if f['path'].endswith('.py')])
            }
        
    def _analyze_dependencies(self, files: List[Dict]) -> Dict:
            """Dependency and package analysis"""
            dep_files = [f for f in files if any(name in f['path'].lower() 
                        for name in ['requirements.txt', 'package.json', 'pom.xml', 'gemfile'])]
            
            if not dep_files:
                return {
                    "issues": [],
                    "total_issues": 0,
                    "message": "No dependency files found"
                }
            
            deps_content = "\n\n".join([f"File: {f['path']}\n{f['content']}" for f in dep_files])
            
            prompt = f"""Analyze these dependencies for issues.

    Dependencies:
    {deps_content}

    Check for:
    1. Outdated packages (check version numbers)
    2. Known security vulnerabilities
    3. Deprecated packages
    4. Missing version pinning
    5. Conflicting dependencies
    6. Unused dependencies

    Return ONLY valid JSON:
    {{
    "issues": [
        {{
        "package": "package-name",
        "current_version": "1.0.0",
        "issue": "Outdated - security vulnerability",
        "severity": "HIGH",
        "recommended_version": "2.5.0",
        "cve": "CVE-2024-1234 if applicable"
        }}
    ],
    "summary": {{
        "total_packages": 0,
        "outdated": 0,
        "vulnerable": 0
    }},
    "total_issues": 0
    }}
    """
            
            try:
                response = self.llm.invoke(prompt)
                content = self._clean_json_response(response.content)
                result = json.loads(content)
                
                if 'total_issues' not in result:
                    result['total_issues'] = len(result.get('issues', []))
                
                return result
            except Exception as e:
                print(f"Dependency analysis error: {e}")
                return {
                    "issues": [],
                    "summary": {},
                    "total_issues": 0
                }
        
    def _analyze_best_practices(self, files: List[Dict]) -> Dict:
            """Best practices and conventions analysis"""
            code_context = self._prepare_code_context(files, max_files=20)
            
            prompt = f"""Check code against modern best practices.

    Code:
    {code_context}

    Evaluate:
    1. Use of modern language features
    2. Async/await usage where appropriate
    3. Context managers for resources
    4. Environment variable usage
    5. Logging practices
    6. Exception handling patterns
    7. Testing presence and coverage
    8. Configuration management
    9. Security best practices
    10. Code organization

    Return ONLY valid JSON:
    {{
    "issues": [
        {{
        "category": "Resource Management",
        "severity": "MEDIUM",
        "issue": "File opened without context manager",
        "file": "filename.py",
        "recommendation": "Use 'with open()' to ensure proper cleanup",
        "example": "with open('file.txt') as f: data = f.read()"
        }}
    ],
    "compliance_score": 75,
    "total_issues": 0
    }}
    """
            
            try:
                response = self.llm.invoke(prompt)
                content = self._clean_json_response(response.content)
                result = json.loads(content)
                
                if 'total_issues' not in result:
                    result['total_issues'] = len(result.get('issues', []))
                
                return result
            except Exception as e:
                print(f"Best practices analysis error: {e}")
                return {
                    "issues": [],
                    "compliance_score": 75,
                    "total_issues": 0
                }
        
    def _calculate_all_scores(self, results: Dict) -> Dict:
            """Calculate accurate scores for all 7 dimensions"""
            scores = {}
            
            # Security Score (0-100)
            sec = results.get('security', {})
            critical = len(sec.get('critical_issues', []))
            high = len(sec.get('high_severity', []))
            medium = len(sec.get('medium_severity', []))
            low = len(sec.get('low_severity', []))
            
            security_score = 100
            security_score -= critical * 20  # Critical is very bad
            security_score -= high * 10
            security_score -= medium * 5
            security_score -= low * 2
            scores['security'] = max(0, min(100, security_score))
            
            # Performance Score (0-100)
            perf = results.get('performance', {})
            perf_issues = perf.get('issues', [])
            severe = len([i for i in perf_issues if i.get('impact') == 'SEVERE'])
            high_perf = len([i for i in perf_issues if i.get('impact') == 'HIGH'])
            
            performance_score = 100
            performance_score -= severe * 15
            performance_score -= high_perf * 8
            performance_score -= (len(perf_issues) - severe - high_perf) * 3
            scores['performance'] = max(0, min(100, performance_score))
            
            # Architecture Score (0-100)
            arch = results.get('architecture', {})
            arch_issues = len(arch.get('issues', []))
            high_arch = len([i for i in arch.get('issues', []) if i.get('severity') == 'HIGH'])
            
            architecture_score = 100
            architecture_score -= high_arch * 12
            architecture_score -= (arch_issues - high_arch) * 5
            scores['architecture'] = max(0, min(100, architecture_score))
            
            # Code Quality Score (0-100)
            quality = results.get('code_quality', {})
            quality_issues = len(quality.get('issues', []))
            
            code_quality_score = 100
            code_quality_score -= quality_issues * 3
            scores['code_quality'] = max(0, min(100, code_quality_score))
            
            # Documentation Score (directly from coverage)
            doc = results.get('documentation', {})
            scores['documentation'] = doc.get('coverage_percentage', 0)
            
            # Dependencies Score (0-100)
            deps = results.get('dependencies', {})
            dep_issues = len(deps.get('issues', []))
            vulnerable = len([i for i in deps.get('issues', []) if 'vulnerab' in i.get('issue', '').lower()])
            
            dependencies_score = 100
            dependencies_score -= vulnerable * 15
            dependencies_score -= (dep_issues - vulnerable) * 5
            scores['dependencies'] = max(0, min(100, dependencies_score))
            
            # Best Practices Score (0-100)
            bp = results.get('best_practices', {})
            bp_issues = len(bp.get('issues', []))
            bp_score = bp.get('compliance_score', 75)
            
            # Use provided score or calculate from issues
            if bp_score:
                scores['best_practices'] = bp_score
            else:
                best_practices_score = 100 - (bp_issues * 4)
                scores['best_practices'] = max(0, min(100, best_practices_score))
            
            # Overall Health Score (weighted average)
            weights = {
                'security': 0.25,
                'performance': 0.15,
                'architecture': 0.15,
                'code_quality': 0.15,
                'documentation': 0.10,
                'dependencies': 0.10,
                'best_practices': 0.10
            }
            
            overall = sum(scores[k] * weights[k] for k in scores)
            scores['overall'] = int(overall)
            
            return scores
        
    def _generate_summary(self, results: Dict) -> Dict:
            """Generate executive summary"""
            scores = results.get('scores', {})
            sec = results.get('security', {})
            
            critical_sec = len(sec.get('critical_issues', []))
            high_sec = len(sec.get('high_severity', []))
            
            # Total issues across all dimensions
            total_issues = sum([
                sec.get('total_issues', 0),
                results.get('performance', {}).get('total_issues', 0),
                results.get('architecture', {}).get('total_issues', 0),
                results.get('code_quality', {}).get('total_issues', 0),
                results.get('documentation', {}).get('total_issues', 0),
                results.get('dependencies', {}).get('total_issues', 0),
                results.get('best_practices', {}).get('total_issues', 0)
            ])
            
            # Generate recommendation
            if critical_sec > 0:
                recommendation = f"🚨 CRITICAL: {critical_sec} critical security vulnerability found! Fix immediately before deployment."
            elif high_sec > 5:
                recommendation = f"⚠ HIGH RISK: {high_sec} high-severity security issues detected. Address urgently."
            elif scores.get('overall', 50) >= 85:
                recommendation = "✅ Excellent! Code quality is very good. Focus on minor improvements."
            elif scores.get('overall', 50) >= 70:
                recommendation = "👍 Good overall quality. Address medium-priority issues for production readiness."
            elif scores.get('overall', 50) >= 50:
                recommendation = "⚠ Needs improvement. Plan a refactoring sprint to address key issues."
            else:
                recommendation = "🚨 Significant issues found. Comprehensive refactoring required before production."
            
            return {
                "total_issues_found": total_issues,
                "critical_security_issues": critical_sec,
                "high_security_issues": high_sec,
                "overall_health_score": scores.get('overall', 0),
                "recommendation": recommendation,
                "dimension_scores": scores
            }
        
    def _prioritize_issues(self, results: Dict) -> List[Dict]:
            """Prioritize top issues across all dimensions"""
            priority_list = []
            
            # Critical security (highest priority)
            for issue in results.get('security', {}).get('critical_issues', [])[:3]:
                priority_list.append({
                    "priority": "CRITICAL",
                    "category": "Security",
                    "issue": issue,
                    "action": "Fix immediately - critical vulnerability"
                })
            
            # High security
            for issue in results.get('security', {}).get('high_severity', [])[:2]:
                priority_list.append({
                    "priority": "HIGH",
                    "category": "Security",
                    "issue": issue,
                    "action": "Fix urgently - high risk"
                })
            
            # Severe performance
            perf_issues = results.get('performance', {}).get('issues', [])
            for issue in [i for i in perf_issues if i.get('impact') == 'SEVERE'][:2]:
                priority_list.append({
                    "priority": "HIGH",
                    "category": "Performance",
                    "issue": issue,
                    "action": "Optimize - severe bottleneck"
                })
            
            # High architecture issues
            arch_issues = results.get('architecture', {}).get('issues', [])
            for issue in [i for i in arch_issues if i.get('severity') == 'HIGH'][:2]:
                priority_list.append({
                    "priority": "MEDIUM",
                    "category": "Architecture",
                    "issue": issue,
                    "action": "Refactor - design improvement needed"
                })
            
            return priority_list[:10]
        
    def _clean_json_response(self, content: str) -> str:
        """Clean response to extract JSON with better error handling"""
        # Remove markdown code blocks
        content = content.replace('`````', '').replace('', '')
        content = content.strip()
        
        # Try to find JSON object
        if '{' in content and '}' in content:
            start = content.find('{')
            end = content.rfind('}')
            json_str = content[start:end+1]
            
            # Validate it's actual JSON
            try:
                json.loads(json_str)
                return json_str
            except:
                pass
        
        # Return empty JSON if parsing fails
        return '{}'
        
    def _prepare_code_context(self, files: List[Dict], max_files: int = 50) -> str:
            """Prepare code for analysis"""
            context_parts = []
            total_chars = 0
            max_chars = 200000
            
            priority_extensions = ['.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.go']
            sorted_files = sorted(files, 
                                key=lambda f: (0 if any(f['path'].endswith(ext) for ext in priority_extensions) else 1))
            
            for file_info in sorted_files[:max_files]:
                if any(file_info['path'].endswith(ext) for ext in priority_extensions):
                    file_str = f"\n{'='*60}\nFile: {file_info['path']}\n{'='*60}\n{file_info['content'][:3000]}\n"
                    
                    if total_chars + len(file_str) > max_chars:
                        break
                    
                    context_parts.append(file_str)
                    total_chars += len(file_str)
            
            return "\n".join(context_parts)
