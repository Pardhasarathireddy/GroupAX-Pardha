"""
Deep Code Analysis Agent - Optimized with Parallel Execution
70% faster with ThreadPoolExecutor
"""
import json
from typing import List, Dict, Any
from langchain_google_genai import ChatGoogleGenerativeAI
import re
from concurrent.futures import ThreadPoolExecutor
import concurrent.futures


class DeepCodeAnalyzer:
    def __init__(self, api_key: str):  # ✅ FIXED: Double underscores
        """Initialize the deep code analyzer"""
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            google_api_key=api_key,
            temperature=0.1,
            max_tokens=8192
        )
    
    def deep_analyze(self, files: List[Dict], repo_structure: str) -> Dict[str, Any]:
        """Perform comprehensive analysis with PARALLEL execution"""
        print("🔍 Starting deep analysis...")
        
        # ⚡ PARALLEL EXECUTION - Run all analyses simultaneously
        with ThreadPoolExecutor(max_workers=5) as executor:
            # Submit all tasks at once
            future_security = executor.submit(self._analyze_security, files)
            future_performance = executor.submit(self._analyze_performance, files)
            future_architecture = executor.submit(self._analyze_architecture, files, repo_structure)
            future_quality = executor.submit(self._analyze_code_quality, files)
            future_deps = executor.submit(self._analyze_dependencies, files)
            future_practices = executor.submit(self._analyze_best_practices, files)
            
            # Collect results as they complete
            results = {}
            
            try:
                results['security'] = future_security.result(timeout=30)
                print("✅ Security complete")
            except Exception as e:
                print(f"⚠ Security failed: {e}")
                results['security'] = {"critical_issues": [], "high_severity": [], "total_issues": 0}
            
            try:
                results['performance'] = future_performance.result(timeout=30)
                print("✅ Performance complete")
            except Exception as e:
                print(f"⚠ Performance failed: {e}")
                results['performance'] = {"issues": [], "total_issues": 0}
            
            try:
                results['architecture'] = future_architecture.result(timeout=30)
                print("✅ Architecture complete")
            except Exception as e:
                print(f"⚠ Architecture failed: {e}")
                results['architecture'] = {"issues": [], "total_issues": 0}
            
            try:
                results['code_quality'] = future_quality.result(timeout=30)
                print("✅ Code quality complete")
            except Exception as e:
                print(f"⚠ Code quality failed: {e}")
                results['code_quality'] = {"issues": [], "total_issues": 0}
            
            try:
                results['dependencies'] = future_deps.result(timeout=30)
                print("✅ Dependencies complete")
            except Exception as e:
                print(f"⚠ Dependencies failed: {e}")
                results['dependencies'] = {"issues": [], "total_issues": 0}
            
            try:
                results['best_practices'] = future_practices.result(timeout=30)
                print("✅ Best practices complete")
            except Exception as e:
                print(f"⚠ Best practices failed: {e}")
                results['best_practices'] = {"issues": [], "total_issues": 0}
        
        # Documentation is fast (no API), run separately
        results['documentation'] = self._analyze_documentation(files)
        print("✅ Documentation complete")
        
        # Calculate scores
        results['scores'] = self._calculate_all_scores(results)
        results['summary'] = self._generate_summary(results)
        results['priority_fixes'] = self._prioritize_issues(results)
        
        return results
    
    def _analyze_security(self, files: List[Dict]) -> Dict:
        """Deep security analysis with hybrid approach"""
        # Pattern-based detection (guaranteed to work)
        pattern_issues = self._detect_security_patterns(files)
        
        # LLM-based analysis
        code_context = self._prepare_code_context(files, max_files=20)  # ⚡ Reduced from 30
        
        if not code_context.strip():
            return {
                "critical_issues": pattern_issues['critical'],
                "high_severity": pattern_issues['high'],
                "medium_severity": [],
                "low_severity": [],
                "total_issues": len(pattern_issues['critical']) + len(pattern_issues['high'])
            }
        
        prompt = f"""Analyze this code for security vulnerabilities. Return findings as a JSON array.

Code:
{code_context[:10000]}

Find these security issues:
1. Hardcoded passwords, API keys, tokens
2. SQL injection vulnerabilities
3. Command injection risks
4. XSS vulnerabilities
5. Path traversal issues

For each issue found, create an entry with:
- file: the filename
- issue: brief description
- severity: CRITICAL, HIGH, MEDIUM, or LOW
- description: detailed explanation
- fix: how to fix it

Return as JSON array:
[
{{
    "file": "app.py",
    "issue": "Hardcoded password",
    "severity": "CRITICAL",
    "description": "Password stored in plaintext",
    "fix": "Use environment variables"
}}
]

Return ONLY the JSON array, nothing else.
"""
        
        try:
            response = self.llm.invoke(prompt)
            content = response.content.strip()
            
            # Try to extract JSON array
            if '[' in content and ']' in content:
                start = content.find('[')
                end = content.rfind(']') + 1
                json_str = content[start:end]
                llm_issues = json.loads(json_str)
            else:
                llm_issues = self._parse_narrative_security(content)
            
            # Categorize LLM findings
            critical_llm = [i for i in llm_issues if i.get('severity') == 'CRITICAL']
            high_llm = [i for i in llm_issues if i.get('severity') == 'HIGH']
            medium_llm = [i for i in llm_issues if i.get('severity') == 'MEDIUM']
            low_llm = [i for i in llm_issues if i.get('severity') == 'LOW']
            
            # Combine with pattern-based findings
            result = {
                "critical_issues": pattern_issues['critical'] + critical_llm,
                "high_severity": pattern_issues['high'] + high_llm,
                "medium_severity": medium_llm,
                "low_severity": low_llm,
                "total_issues": 0
            }
            
            result['total_issues'] = sum(len(result[k]) for k in 
                ['critical_issues', 'high_severity', 'medium_severity', 'low_severity'])
            
            print(f"✅ Security: {result['total_issues']} total issues found")
            return result
            
        except Exception as e:
            print(f"⚠ LLM parsing failed: {e}, using pattern detection only")
            return {
                "critical_issues": pattern_issues['critical'],
                "high_severity": pattern_issues['high'],
                "medium_severity": [],
                "low_severity": [],
                "total_issues": len(pattern_issues['critical']) + len(pattern_issues['high'])
            }
    
    def _analyze_performance(self, files: List[Dict]) -> Dict:
        """Performance analysis with specific bottlenecks"""
        code_context = self._prepare_code_context(files, max_files=20)  # ⚡ Reduced
        
        prompt = f"""Analyze this code for performance issues.

Code:
{code_context[:10000]}

Find performance problems:
1. Inefficient algorithms (O(n²) or worse)
2. Database query issues
3. Memory leaks
4. Blocking I/O
5. Missing caching

Return ONLY valid JSON:
{{
  "issues": [
    {{
      "file": "filename.py",
      "location": "function_name",
      "impact": "SEVERE",
      "issue": "Nested loops causing O(n²)",
      "fix": "Use set or dict for O(1) lookups"
    }}
  ],
  "total_issues": 0
}}
"""
        
        try:
            response = self.llm.invoke(prompt)
            content = self._clean_json_response(response.content)
            result = json.loads(content)
            
            if 'total_issues' not in result:
                result['total_issues'] = len(result.get('issues', []))
            
            return result
        except Exception as e:
            print(f"Performance analysis error: {e}")
            return {"issues": [], "total_issues": 0}
    
    def _detect_security_patterns(self, files: List[Dict]) -> Dict:
        """Pattern-based security detection (always works)"""
        critical = []
        high = []
        
        for file_info in files[:30]:  # ⚡ Limit files
            if not file_info['path'].endswith(('.py', '.js', '.java', '.php')):
                continue
                
            content = file_info['content']
            filepath = file_info['path']
            
            # Hardcoded credentials
            if re.search(r'(password|passwd|pwd)\s*=\s*["\'][^"\']+["\']', content, re.IGNORECASE):
                critical.append({
                    "file": filepath,
                    "issue": "Hardcoded password detected",
                    "severity": "CRITICAL",
                    "description": "Password stored in plaintext",
                    "fix": "Use environment variables"
                })
            
            if re.search(r'(api[-]?key|apikey|secret[-]?key)\s*=\s*["\'][^"\']+["\']', content, re.IGNORECASE):
                critical.append({
                    "file": filepath,
                    "issue": "Hardcoded API key",
                    "severity": "CRITICAL",
                    "description": "API key exposed",
                    "fix": "Use environment variables"
                })
            
            # SQL Injection
            if re.search(r'(execute|cursor\.execute|query)\s*\(\s*f["\']', content):
                critical.append({
                    "file": filepath,
                    "issue": "SQL injection vulnerability",
                    "severity": "CRITICAL",
                    "description": "String formatting in SQL",
                    "fix": "Use parameterized queries"
                })
            
            # Command Injection
            if re.search(r'os\.system|subprocess\.call.*shell=True', content):
                high.append({
                    "file": filepath,
                    "issue": "Command injection risk",
                    "severity": "HIGH",
                    "description": "Direct command execution",
                    "fix": "Use subprocess with shell=False"
                })
            
            # Eval usage
            if re.search(r'\beval\s*\(', content):
                high.append({
                    "file": filepath,
                    "issue": "Dangerous eval() usage",
                    "severity": "HIGH",
                    "description": "eval() executes arbitrary code",
                    "fix": "Use ast.literal_eval()"
                })
        
        return {'critical': critical, 'high': high}
    
    def _parse_narrative_security(self, narrative: str) -> List[Dict]:
        """Parse narrative security response"""
        issues = []
        lines = narrative.split('\n')
        current_issue = {}
        
        for line in lines:
            line = line.strip()
            if not line:
                if current_issue:
                    issues.append(current_issue)
                    current_issue = {}
                continue
            
            if 'critical' in line.lower() or 'hardcoded' in line.lower():
                current_issue['severity'] = 'CRITICAL'
            elif 'high' in line.lower() or 'injection' in line.lower():
                current_issue['severity'] = 'HIGH'
            
            if 'password' in line.lower():
                current_issue['issue'] = 'Hardcoded password'
                current_issue['description'] = line
            elif 'api' in line.lower() and 'key' in line.lower():
                current_issue['issue'] = 'Hardcoded API key'
                current_issue['description'] = line
            elif 'sql' in line.lower() and 'injection' in line.lower():
                current_issue['issue'] = 'SQL injection'
                current_issue['description'] = line
            
            if 'issue' in current_issue and 'file' not in current_issue:
                current_issue['file'] = 'Multiple files'
                current_issue['fix'] = 'See description'
        
        if current_issue:
            issues.append(current_issue)
        
        return issues
    
    def _analyze_architecture(self, files: List[Dict], repo_structure: str) -> Dict:
        """Architecture analysis"""
        code_context = self._prepare_code_context(files, max_files=15)  # ⚡ Reduced
        
        prompt = f"""Analyze software architecture.

Structure:
{repo_structure[:500]}

Code:
{code_context[:8000]}

Return ONLY valid JSON:
{{
  "architecture_pattern": "MVC/Layered/Unknown",
  "issues": [
    {{
      "category": "SOLID Violation",
      "severity": "HIGH",
      "issue": "Single Responsibility violated",
      "file": "filename.py",
      "recommendation": "Split into separate classes"
    }}
  ],
  "total_issues": 0
}}
"""
        
        try:
            response = self.llm.invoke(prompt)
            content = self._clean_json_response(response.content)
            result = json.loads(content)
            
            if 'total_issues' not in result:
                result['total_issues'] = len(result.get('issues', []))
            
            return result
        except Exception as e:
            print(f"Architecture error: {e}")
            return {"architecture_pattern": "Unknown", "issues": [], "total_issues": 0}
    
    def _analyze_code_quality(self, files: List[Dict]) -> Dict:
        """Code quality analysis"""
        code_context = self._prepare_code_context(files, max_files=20)  # ⚡ Reduced
        
        prompt = f"""Analyze code quality.

Code:
{code_context[:10000]}

Find issues:
1. Code smells
2. Complexity
3. Naming conventions
4. Magic numbers
5. Dead code

Return ONLY valid JSON:
{{
  "issues": [
    {{
      "file": "filename.py",
      "type": "code_smell",
      "severity": "MEDIUM",
      "issue": "Function too long",
      "suggestion": "Break into smaller functions"
    }}
  ],
  "total_issues": 0
}}
"""
        
        try:
            response = self.llm.invoke(prompt)
            content = self._clean_json_response(response.content)
            result = json.loads(content)
            
            if 'total_issues' not in result:
                result['total_issues'] = len(result.get('issues', []))
            
            return result
        except Exception as e:
            print(f"Code quality error: {e}")
            return {"issues": [], "total_issues": 0}
    
    def _analyze_documentation(self, files: List[Dict]) -> Dict:
        """Documentation coverage analysis"""
        total_functions = 0
        documented_functions = 0
        files_needing_docs = []
        
        for file_info in files[:40]:  # ⚡ Limit files
            if not file_info['path'].endswith('.py'):
                continue
                
            content = file_info['content']
            functions = re.findall(r'def\s+(\w+)\s*\([^)]*\):', content)
            total_functions += len(functions)
            
            for func in functions:
                pattern = rf'def\s+{func}\s*\([^)]\):\s(?:"""|\'\'\')[^"\']*(?:"""|\'\'\')'
                if re.search(pattern, content, re.DOTALL):
                    documented_functions += 1
                else:
                    files_needing_docs.append({
                        "file": file_info['path'],
                        "function": func,
                        "issue": "Missing docstring"
                    })
        
        coverage = int((documented_functions / total_functions * 100)) if total_functions > 0 else 100
        
        return {
            "coverage_percentage": coverage,
            "total_functions": total_functions,
            "documented_functions": documented_functions,
            "undocumented_functions": total_functions - documented_functions,
            "issues": files_needing_docs[:20],
            "total_issues": len(files_needing_docs)
        }
    
    def _analyze_dependencies(self, files: List[Dict]) -> Dict:
        """Dependency analysis"""
        dep_files = [f for f in files if any(name in f['path'].lower() 
                    for name in ['requirements.txt', 'package.json', 'pom.xml'])]
        
        if not dep_files:
            return {"issues": [], "total_issues": 0, "message": "No dependency files"}
        
        deps_content = "\n\n".join([f"File: {f['path']}\n{f['content']}" for f in dep_files])
        
        prompt = f"""Analyze dependencies.

Dependencies:
{deps_content[:5000]}

Check for outdated or vulnerable packages.

Return ONLY valid JSON:
{{
  "issues": [
    {{
      "package": "package-name",
      "current_version": "1.0.0",
      "issue": "Outdated",
      "severity": "HIGH",
      "recommended_version": "2.0.0"
    }}
  ],
  "total_issues": 0
}}
"""
        
        try:
            response = self.llm.invoke(prompt)
            content = self._clean_json_response(response.content)
            result = json.loads(content)
            
            if 'total_issues' not in result:
                result['total_issues'] = len(result.get('issues', []))
            
            return result
        except Exception as e:
            print(f"Dependencies error: {e}")
            return {"issues": [], "total_issues": 0}
    
    def _analyze_best_practices(self, files: List[Dict]) -> Dict:
        """Best practices analysis"""
        code_context = self._prepare_code_context(files, max_files=15)  # ⚡ Reduced
        
        prompt = f"""Check best practices.

Code:
{code_context[:8000]}

Return ONLY valid JSON:
{{
  "issues": [
    {{
      "category": "Resource Management",
      "severity": "MEDIUM",
      "issue": "File opened without context manager",
      "file": "filename.py",
      "recommendation": "Use 'with open()'"
    }}
  ],
  "compliance_score": 75,
  "total_issues": 0
}}
"""
        
        try:
            response = self.llm.invoke(prompt)
            content = self._clean_json_response(response.content)
            result = json.loads(content)
            
            if 'total_issues' not in result:
                result['total_issues'] = len(result.get('issues', []))
            
            return result
        except Exception as e:
            print(f"Best practices error: {e}")
            return {"issues": [], "compliance_score": 75, "total_issues": 0}
    
    def _calculate_all_scores(self, results: Dict) -> Dict:
        """Calculate accurate scores for all 7 dimensions"""
        scores = {}
        
        # Security Score
        sec = results.get('security', {})
        critical = len(sec.get('critical_issues', []))
        high = len(sec.get('high_severity', []))
        medium = len(sec.get('medium_severity', []))
        low = len(sec.get('low_severity', []))
        
        security_score = 100 - (critical * 20) - (high * 10) - (medium * 5) - (low * 2)
        scores['security'] = max(0, min(100, security_score))
        
        # Performance Score
        perf = results.get('performance', {})
        perf_issues = perf.get('issues', [])
        severe = len([i for i in perf_issues if i.get('impact') == 'SEVERE'])
        high_perf = len([i for i in perf_issues if i.get('impact') == 'HIGH'])
        
        performance_score = 100 - (severe * 15) - (high_perf * 8) - ((len(perf_issues) - severe - high_perf) * 3)
        scores['performance'] = max(0, min(100, performance_score))
        
        # Architecture Score
        arch = results.get('architecture', {})
        arch_issues = len(arch.get('issues', []))
        high_arch = len([i for i in arch.get('issues', []) if i.get('severity') == 'HIGH'])
        
        architecture_score = 100 - (high_arch * 12) - ((arch_issues - high_arch) * 5)
        scores['architecture'] = max(0, min(100, architecture_score))
        
        # Code Quality Score
        quality = results.get('code_quality', {})
        quality_issues = len(quality.get('issues', []))
        scores['code_quality'] = max(0, min(100, 100 - (quality_issues * 3)))
        
        # Documentation Score
        doc = results.get('documentation', {})
        scores['documentation'] = doc.get('coverage_percentage', 0)
        
        # Dependencies Score
        deps = results.get('dependencies', {})
        dep_issues = len(deps.get('issues', []))
        vulnerable = len([i for i in deps.get('issues', []) if 'vulnerab' in i.get('issue', '').lower()])
        scores['dependencies'] = max(0, min(100, 100 - (vulnerable * 15) - ((dep_issues - vulnerable) * 5)))
        
        # Best Practices Score
        bp = results.get('best_practices', {})
        scores['best_practices'] = bp.get('compliance_score', 75)
        
        # Overall Health Score
        weights = {'security': 0.25, 'performance': 0.15, 'architecture': 0.15,
                  'code_quality': 0.15, 'documentation': 0.10, 'dependencies': 0.10,
                  'best_practices': 0.10}
        
        overall = sum(scores[k] * weights[k] for k in scores)
        scores['overall'] = int(overall)
        
        return scores
    
    def _generate_summary(self, results: Dict) -> Dict:
        """Generate executive summary"""
        scores = results.get('scores', {})
        sec = results.get('security', {})
        
        critical_sec = len(sec.get('critical_issues', []))
        high_sec = len(sec.get('high_severity', []))
        
        total_issues = sum([
            sec.get('total_issues', 0),
            results.get('performance', {}).get('total_issues', 0),
            results.get('architecture', {}).get('total_issues', 0),
            results.get('code_quality', {}).get('total_issues', 0),
            results.get('documentation', {}).get('total_issues', 0),
            results.get('dependencies', {}).get('total_issues', 0),
            results.get('best_practices', {}).get('total_issues', 0)
        ])
        
        # Generate recommendation
        if critical_sec > 0:
            recommendation = f"🚨 CRITICAL: {critical_sec} critical security vulnerabilities! Fix immediately."
        elif high_sec > 5:
            recommendation = f"⚠ HIGH RISK: {high_sec} high-severity issues. Address urgently."
        elif scores.get('overall', 50) >= 85:
            recommendation = "✅ Excellent! Code quality is very good."
        elif scores.get('overall', 50) >= 70:
            recommendation = "👍 Good overall quality."
        else:
            recommendation = "⚠ Needs improvement."
        
        return {
            "total_issues_found": total_issues,
            "critical_security_issues": critical_sec,
            "high_security_issues": high_sec,
            "performance_bottlenecks": len(results.get('performance', {}).get('issues', [])),
            "overall_health_score": scores.get('overall', 0),
            "recommendation": recommendation,
            "dimension_scores": scores
        }
    
    def _prioritize_issues(self, results: Dict) -> List[Dict]:
        """Prioritize top issues"""
        priority_list = []
        
        # Critical security
        for issue in results.get('security', {}).get('critical_issues', [])[:3]:
            priority_list.append({
                "priority": "CRITICAL",
                "category": "Security",
                "issue": issue,
                "action": "Fix immediately"
            })
        
        # High security
        for issue in results.get('security', {}).get('high_severity', [])[:2]:
            priority_list.append({
                "priority": "HIGH",
                "category": "Security",
                "issue": issue,
                "action": "Fix urgently"
            })
        
        return priority_list[:10]
    
    def _clean_json_response(self, content: str) -> str:
        """Clean response to extract JSON"""
        content = content.replace('``````', '').strip()
        
        if '{' in content and '}' in content:
            start = content.find('{')
            end = content.rfind('}')
            json_str = content[start:end+1]
            
            try:
                json.loads(json_str)
                return json_str
            except:
                pass
        
        return '{}'
    
    def _prepare_code_context(self, files: List[Dict], max_files: int = 30) -> str:  # ⚡ Default reduced
        """Prepare code for analysis"""
        context_parts = []
        total_chars = 0
        max_chars = 150000  # ⚡ Reduced from 200000
        
        priority_extensions = ['.py', '.js', '.ts', '.jsx', '.tsx', '.java']
        sorted_files = sorted(files, 
                            key=lambda f: (0 if any(f['path'].endswith(ext) for ext in priority_extensions) else 1))
        
        for file_info in sorted_files[:max_files]:
            if any(file_info['path'].endswith(ext) for ext in priority_extensions):
                file_str = f"\nFile: {file_info['path']}\n{file_info['content'][:2000]}\n"  # ⚡ Reduced from 3000
                
                if total_chars + len(file_str) > max_chars:
                    break
                
                context_parts.append(file_str)
                total_chars += len(file_str)
        
        return "\n".join(context_parts)
